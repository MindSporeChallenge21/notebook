{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f82204-a12f-48bc-a5ea-7e2b8e9b108c",
   "metadata": {},
   "source": [
    "# MindSpore Challenge Tutorial (Beginner)\n",
    "\n",
    "Throughout this tutorial we will cover [MindSpore](https://www.mindspore.cn/doc/api_python/en/r1.2/index.html)'s common modules\n",
    "- MindSpore Dataset [mindspore.dataset][ms dataset]\n",
    "- MindSpore Neural Network [mindspore.nn][ms nn]\n",
    "- MindSpore Operators [mindspore.ops][ms ops]\n",
    "- Training and creating MindSpore Models.\n",
    "\n",
    "Prerequisite:\n",
    "- Understanding of Python and Python common packages\n",
    "- Understanding of operations and  of common machine learning libraries (Nice to have)\n",
    "- Understanding of designing, training, evaluation of machine learning libraries (Nice to have)\n",
    "\n",
    "We will be referencing from [MindSpore Course](https://www.mindspore.cn/tutorial/zh-CN/r1.2/quick_start.html)'s LeNet5 on MNIST Dataset\n",
    "\n",
    "[ms dataset]: https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/mindspore.dataset.html\n",
    "[ms nn]: https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/mindspore.nn.html\n",
    "[ms ops]: https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/mindspore.ops.html\n",
    "\n",
    "## Obtaining notebook https://github.com/MindSporeChallenge21/notebook\n",
    "Visit the link above to download this notebook so you can try on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d493557-49cf-4b23-8fe2-c14f88fabc0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaababef-7326-4989-a555-3cbc7f972814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from copy import copy\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from easydict import EasyDict as ed\n",
    "from scipy import interpolate\n",
    "\n",
    "import requests\n",
    "\n",
    "import mindspore as ms\n",
    "from mindspore import nn\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore import dataset\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage.io import imread_collection\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d30fd2-f086-4835-9a63-7934d7d038a3",
   "metadata": {},
   "source": [
    "## MindSpore Basics\n",
    "\n",
    "1. Creating a [MindSpore Dataset][ms dataset] object\n",
    "2. Creating a [MindSpore Neural Network][ms nn]\n",
    "3. Training the [MindSpore Neural Network][ms nn]\n",
    "\n",
    "[ms dataset]: https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/mindspore.dataset.html\n",
    "[ms nn]: https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/mindspore.nn.html\n",
    "[ms ops]: https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/mindspore.ops.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7213f4c-61d0-4c57-9b76-b564c0561bd9",
   "metadata": {},
   "source": [
    "## MindSpore Context\n",
    "\n",
    "You need to specify the type of the backend you are going to use, namely: `CPU`, `GPU`, `Ascend`.\n",
    "\n",
    "`Ascend` is a Neural Processing Unit(NPU) developed by Huawei, it will significantly speed up the process of training a model, which we will try later.\n",
    "\n",
    "For the mode of execution, there is two choices, `GRAPH_MODE` or `PYNATIVE_MODE`.\n",
    "\n",
    "`GRAPH_MODE` are designed for training purposes, with faster execution performance, but is generally harder for development.\n",
    "\n",
    "`PYNATIVE_MODE` are design for development purposes, with slower execution performance, but allows neural network to easily modified.\n",
    "\n",
    "For more information see [`mindspore.context.set_context`](https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/mindspore.context.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ae761d-d17b-49cd-9128-5a24eb91f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.context.set_context(device_target='CPU', mode=ms.context.PYNATIVE_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bce79a-43bd-48ea-bfde-814337dc390f",
   "metadata": {},
   "source": [
    "\n",
    "## Downloading MNIST Data and creating a MindSpore Dataset object.\n",
    "\n",
    "We can create a MindSpore Dataset object with the [mindspore.dataset][ms dataset] module. \n",
    "\n",
    "There is a [MnistDataset][ms mnist dataset] class for importing MnistDatasets.\n",
    "\n",
    "[ms dataset]: https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/mindspore.dataset.html\n",
    "[ms mnist dataset]: https://www.mindspore.cn/doc/api_python/zh-CN/r1.2/mindspore/dataset/mindspore.dataset.MnistDataset.html#mindspore.dataset.MnistDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd0fb84-d212-42a9-b46a-ee47eda3694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-24 16:39:21--  https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-labels-idx1-ubyte\n",
      "Resolving mindspore-website.obs.myhuaweicloud.com (mindspore-website.obs.myhuaweicloud.com)... 49.4.112.91, 49.4.112.92, 49.4.112.3\n",
      "Connecting to mindspore-website.obs.myhuaweicloud.com (mindspore-website.obs.myhuaweicloud.com)|49.4.112.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘./datasets/MNIST_Data/train/train-labels-idx1-ubyte’ not modified on server. Omitting download.\n",
      "\n",
      "--2021-08-24 16:39:21--  https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-images-idx3-ubyte\n",
      "Resolving mindspore-website.obs.myhuaweicloud.com (mindspore-website.obs.myhuaweicloud.com)... 49.4.112.91, 49.4.112.92, 49.4.112.3\n",
      "Connecting to mindspore-website.obs.myhuaweicloud.com (mindspore-website.obs.myhuaweicloud.com)|49.4.112.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘./datasets/MNIST_Data/train/train-images-idx3-ubyte’ not modified on server. Omitting download.\n",
      "\n",
      "--2021-08-24 16:39:21--  https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-labels-idx1-ubyte\n",
      "Resolving mindspore-website.obs.myhuaweicloud.com (mindspore-website.obs.myhuaweicloud.com)... 49.4.112.91, 49.4.112.92, 49.4.112.3\n",
      "Connecting to mindspore-website.obs.myhuaweicloud.com (mindspore-website.obs.myhuaweicloud.com)|49.4.112.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘./datasets/MNIST_Data/test/t10k-labels-idx1-ubyte’ not modified on server. Omitting download.\n",
      "\n",
      "--2021-08-24 16:39:22--  https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-images-idx3-ubyte\n",
      "Resolving mindspore-website.obs.myhuaweicloud.com (mindspore-website.obs.myhuaweicloud.com)... 49.4.112.91, 49.4.112.92, 49.4.112.3\n",
      "Connecting to mindspore-website.obs.myhuaweicloud.com (mindspore-website.obs.myhuaweicloud.com)|49.4.112.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘./datasets/MNIST_Data/test/t10k-images-idx3-ubyte’ not modified on server. Omitting download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./datasets/MNIST_Data/train ./datasets/MNIST_Data/test\n",
    "!wget -NP ./datasets/MNIST_Data/train https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-labels-idx1-ubyte\n",
    "!wget -NP ./datasets/MNIST_Data/train https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-images-idx3-ubyte\n",
    "!wget -NP ./datasets/MNIST_Data/test https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-labels-idx1-ubyte\n",
    "!wget -NP ./datasets/MNIST_Data/test https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-images-idx3-ubyte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc8e188-3455-418e-80ab-71166ef5496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset_path = Path('datasets') / 'MNIST_Data'\n",
    "\n",
    "mnist_train_dataset_path = mnist_dataset_path / 'train'\n",
    "mnist_test_dataset_path = mnist_dataset_path / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "948f5a3b-64b0-4368-8551-c8d1efbafb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "\n",
    "# creating a mindspore dataset\n",
    "def create_dataset(mnist_dataset_path, batch_size=32, num_parallel_workers=2):\n",
    "    \n",
    "    # use MnistDataset to read the donwloaded data\n",
    "    dataset = ms.dataset.MnistDataset(str(mnist_train_dataset_path))\n",
    "    \n",
    "    # define variables\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "\n",
    "    # define operations\n",
    "    rescale_op = CV.Rescale(rescale, shift)\n",
    "    hwc2chw_op = CV.HWC2CHW()\n",
    "    type_cast_op = C.TypeCast(ms.dtype.int32)\n",
    "\n",
    "    # labels to int32\n",
    "    dataset = dataset.map(\n",
    "        operations=type_cast_op, \n",
    "        input_columns=\"label\", \n",
    "        num_parallel_workers=num_parallel_workers\n",
    "    )\n",
    "    \n",
    "    # image values to [0,1]\n",
    "    # image from HWC to CHW  <- requirements for convolution\n",
    "    dataset = dataset.map(\n",
    "        operations=[rescale_op, hwc2chw_op], \n",
    "        input_columns=\"image\", \n",
    "        num_parallel_workers=num_parallel_workers\n",
    "    )\n",
    "    \n",
    "    # perform shuffling and batching\n",
    "    buffer_size = 10000\n",
    "    dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028ca97e-05c6-46f3-a32c-f3370d4a5bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist_train_ds = create_dataset(mnist_train_dataset_path, 32)\n",
    "mnist_test_ds = create_dataset(mnist_test_dataset_path, 32)\n",
    "\n",
    "for i in mnist_train_ds.create_dict_iterator():\n",
    "    plt.imshow(i['image'].asnumpy()[0,0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae0b1a60-7d2d-4e49-9644-48dada0b39a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size: 1875 Testing Dataset Size: 1875\n"
     ]
    }
   ],
   "source": [
    "print('Train Dataset Size:', mnist_train_ds.get_dataset_size(), 'Testing Dataset Size:', mnist_test_ds.get_dataset_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0aadb3-a5f9-4166-a45b-7159eb42d1d6",
   "metadata": {},
   "source": [
    "## Defining a Neural Network (LeNet5)\n",
    "\n",
    "![](https://segmentfault.com/img/remote/1460000023074059)\n",
    "\n",
    "1. 2 Convolution layer (kernel: 5x5)\n",
    "2. 3 Dense layer of\n",
    "\n",
    "To define network we need to define a class which extends `nn.Cell`.\n",
    "\n",
    "This class have two parts, the `__init__` and the `construct` function:\n",
    "- `__init__` all neural network operators will be defined in this function.\n",
    "- `construct` all neural network graph will be defined in this function.\n",
    "\n",
    "The common neural network layers are the `mindspore.nn.Conv2D`, `mindspore.nn.Dense`, to use them correctly user are required to define the input and output channels/nodes for these function.\n",
    "\n",
    "The [mindspore.nn.Conv2d](https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/nn/mindspore.nn.Conv2d.html#mindspore.nn.Conv2d) requires the **kernel size**, the **input channel** and **output channel** sizes. The input channels are based on the number of channels in the images or in the intermediate steps, and the output channels can be freely defined. \n",
    "\n",
    "The [mindspore.nn.Dense](https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/nn/mindspore.nn.Dense.html#mindspore.nn.Dense) (Fully connected layers) requires the **numbers of input node** and the **numbers of output nodes**. The number of input nodes are based on the nodes in the previous layers, usually we will be flattening the results from the convolution layers and pooling layers to use in the dense layer, some calculation maybe needed to find the number of nodes after the flattenning result, usually this is the number of channels * the width and height of each channels. \n",
    "\n",
    "Other common operators can be found in the [following list](https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/mindspore.nn.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1bb9fb4-a2cd-469e-8b24-38acf59d5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import nn\n",
    "from mindspore.common.initializer import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84ab903-54e5-4d57-8508-42fd3e36ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_image_shape = 28 # assume image is square\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    \n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')  # stride is 1\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid') \n",
    "        \n",
    "        self.fc1 = nn.Dense(16 * 4 * 4, 120, weight_init=Normal(0.02))\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
    "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.shape = ms.ops.Shape()\n",
    "        self.print = ms.ops.Print()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x) # this operation is static, so we can use it multiple times\n",
    "        x = self.max_pool2d(x)  # this operation is static, so we can use it multiple times\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# creating an instance of LeNet5\n",
    "net = LeNet5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd214a-1fdd-4779-84ec-8241ba3cfbbb",
   "metadata": {},
   "source": [
    "_To use these operations, one may need to understand how to calculate the number of nodes after convolution steps, moreover, you can use the [`mindspore.ops.Shape`](https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/ops/mindspore.ops.Shape.html#mindspore.ops.Shape) and [`mindspore.ops.Print`](https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/ops/mindspore.ops.Print.html#mindspore.ops.Print) to find out the shapes of the outputs._\n",
    "\n",
    "Knowing how to do the math for calculating number of nodes/image size are very useful to have a deep understanding of the convolution steps and fully-connected steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b930a-889e-49ae-8a4e-3f030ec5d23e",
   "metadata": {},
   "source": [
    "You might confused on how `16 * 4 * 4` is used as the input of the first dense layer.\n",
    "\n",
    "The below class calculates the input and output channels and nodes dynamically and design the model that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15cb58f-88b8-4a80-af3d-6928e1ba3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_shape = 28 # assume image is square\n",
    "\n",
    "class LeNet5_dynamic(nn.Cell):\n",
    "    \n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5_dynamic, self).__init__()\n",
    "        \n",
    "        conv_channels_1 = 6\n",
    "        conv_channels_2 = 16\n",
    "        \n",
    "        kernel_size_1 = kernel_size_2 = 5\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_channel, conv_channels_1, kernel_size_1, pad_mode='valid')  # stride is 1\n",
    "        self.conv2 = nn.Conv2d(conv_channels_1, conv_channels_2, kernel_size_2, pad_mode='valid')  # stride is 1\n",
    "        \n",
    "        conv_output_size_1 = math.ceil((input_image_shape - (kernel_size_1-1)) / 1) # (input image shape - (kernel size - 1)) / stride\n",
    "        max_pooling_output_size_1 = math.ceil((conv_output_size_1 - (2-1)) / 2) #  1 + (conv 1 image shape - (kernel size - 1)) / stride\n",
    "        conv_output_size_2 = math.ceil((max_pooling_output_size_1 - (kernel_size_2-1)) / 1)  # \n",
    "        max_pooling_output_size_2 = math.ceil((conv_output_size_2 - (2-1)) / 2)  # \n",
    "        \n",
    "        dense_nodes_1 = 120\n",
    "        dense_nodes_2 = 84\n",
    "        \n",
    "        self.fc1 = nn.Dense(conv_channels_2 * max_pooling_output_size_2 * max_pooling_output_size_2, dense_nodes_1, weight_init=Normal(0.02))\n",
    "        self.fc2 = nn.Dense(dense_nodes_1, dense_nodes_2, weight_init=Normal(0.02))\n",
    "        self.fc3 = nn.Dense(dense_nodes_2, num_class, weight_init=Normal(0.02))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.shape = ms.ops.Shape()\n",
    "        self.print = ms.ops.Print()\n",
    "\n",
    "    def construct(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        self.print(self.shape(x))  # use print and shape to find out the shape if you are uncertain.\n",
    "        \n",
    "        x = self.max_pool2d(x)\n",
    "        self.print(self.shape(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)  \n",
    "        self.print(self.shape(x))\n",
    "        \n",
    "        x = self.max_pool2d(x) # this operation is static, so we can use it multiple times\n",
    "        self.print(self.shape(x))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        self.print(self.shape(x))\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbcdf5fb-07cc-41d9-b096-378a1c452426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[32, 10], dtype=Float32, value=\n",
       "[[ 2.91483193e-05, -4.27754549e-06,  2.95517361e-06 ...  9.11741367e-08, -1.25127917e-05, -6.28899079e-06],\n",
       " [ 3.04563637e-05, -4.71644853e-06, -2.23849347e-05 ...  1.38128789e-05, -1.66006685e-05,  2.93739788e-08],\n",
       " [ 2.52394311e-05, -1.41289584e-05,  1.64396454e-06 ...  1.71922443e-06, -6.12040412e-06, -1.32128262e-05],\n",
       " ...\n",
       " [ 2.75114398e-05, -3.64461812e-06, -7.15348460e-06 ... -1.04310050e-06, -9.15611326e-06, -5.34610172e-06],\n",
       " [ 3.38642640e-05,  4.48467699e-06, -5.02298053e-06 ... -1.01861297e-05, -6.05442256e-06, -8.73958470e-06],\n",
       " [ 2.12739924e-05, -8.54431346e-06,  1.08481709e-05 ...  1.04173314e-05, -3.89720253e-06, -1.01335017e-05]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(i['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13adf0c6-5a00-4202-89c1-94ed7d04837f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining a Model\n",
    "\n",
    "The models in MindSpore requires 3 parameters\n",
    "- the network\n",
    "- the optimizer [MindSpore Optimizer Functions](https://www.mindspore.cn/doc/api_python/zh-CN/r1.2/mindspore/mindspore.nn.html#optimizer-functions)\n",
    "- the loss function [MindSpore Loss Functions](https://www.mindspore.cn/doc/api_python/zh-CN/r1.2/mindspore/mindspore.nn.html#loss-functions)\n",
    "\n",
    "The optimizers is the method to reduces the loss, and the loss function measures the evaluation performance of the network.\n",
    "\n",
    "The optimizers have one required parameters which is the network's parameters. These parameters, can be obtained by `net.trainable_params()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "699e135d-d973-437e-a07a-f34ad41fe842",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ms.nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "optimizers = ms.nn.Adam(net.trainable_params(), learning_rate=0.01)\n",
    "\n",
    "model = ms.Model(\n",
    "    net,\n",
    "    loss,\n",
    "    optimizers, \n",
    "    metrics={\"Accuracy\": ms.nn.Accuracy()}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc0c53-ce8a-465f-8773-5b09a4297d22",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We might want to monitor the _loss changes_, and _create checkpoints_ for model at few epoches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac61402d-7090-4d4b-bd93-afcf3d400c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(3023:139781394663040,MainProcess):2021-08-20-14:40:18.431.168 [mindspore/train/model.py:412] The CPU cannot support dataset sink mode currently.So the training process will be performed with dataset not sink.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 125, loss is 0.521804\n",
      "epoch: 1 step: 250, loss is 0.23421651\n",
      "epoch: 1 step: 375, loss is 0.21887353\n",
      "epoch: 1 step: 500, loss is 0.06689039\n",
      "epoch: 1 step: 625, loss is 0.18427612\n",
      "epoch: 1 step: 750, loss is 0.1372671\n",
      "epoch: 1 step: 875, loss is 0.2118403\n",
      "epoch: 1 step: 1000, loss is 0.0872464\n",
      "epoch: 1 step: 1125, loss is 0.08675042\n",
      "epoch: 1 step: 1250, loss is 0.2049871\n",
      "epoch: 1 step: 1375, loss is 0.1300338\n",
      "epoch: 1 step: 1500, loss is 0.09513578\n",
      "epoch: 1 step: 1625, loss is 0.057195194\n",
      "epoch: 1 step: 1750, loss is 0.060339466\n",
      "epoch: 1 step: 1875, loss is 0.12788846\n",
      "epoch: 2 step: 125, loss is 0.08784718\n",
      "epoch: 2 step: 250, loss is 0.07043126\n",
      "epoch: 2 step: 375, loss is 0.00695764\n",
      "epoch: 2 step: 500, loss is 0.31245264\n",
      "epoch: 2 step: 625, loss is 0.15288578\n",
      "epoch: 2 step: 750, loss is 0.32073644\n",
      "epoch: 2 step: 875, loss is 0.18298802\n",
      "epoch: 2 step: 1000, loss is 0.09131481\n",
      "epoch: 2 step: 1125, loss is 0.17523211\n",
      "epoch: 2 step: 1250, loss is 0.17023318\n",
      "epoch: 2 step: 1375, loss is 0.0017596555\n",
      "epoch: 2 step: 1500, loss is 0.13824458\n",
      "epoch: 2 step: 1625, loss is 0.029102629\n",
      "epoch: 2 step: 1750, loss is 0.260452\n",
      "epoch: 2 step: 1875, loss is 0.023786385\n",
      "epoch: 3 step: 125, loss is 0.11278246\n",
      "epoch: 3 step: 250, loss is 0.07250986\n",
      "epoch: 3 step: 375, loss is 0.04683754\n",
      "epoch: 3 step: 500, loss is 0.0133730285\n",
      "epoch: 3 step: 625, loss is 0.003933675\n",
      "epoch: 3 step: 750, loss is 0.044776224\n",
      "epoch: 3 step: 875, loss is 0.46108177\n",
      "epoch: 3 step: 1000, loss is 0.18441027\n",
      "epoch: 3 step: 1125, loss is 0.016294181\n",
      "epoch: 3 step: 1250, loss is 0.077295534\n",
      "epoch: 3 step: 1375, loss is 0.015679017\n",
      "epoch: 3 step: 1500, loss is 0.4801179\n",
      "epoch: 3 step: 1625, loss is 0.09163268\n",
      "epoch: 3 step: 1750, loss is 0.053082883\n",
      "epoch: 3 step: 1875, loss is 0.00304176\n",
      "epoch: 4 step: 125, loss is 0.41182318\n",
      "epoch: 4 step: 250, loss is 0.097466454\n",
      "epoch: 4 step: 375, loss is 0.00080468034\n",
      "epoch: 4 step: 500, loss is 0.1365189\n",
      "epoch: 4 step: 625, loss is 0.009699042\n",
      "epoch: 4 step: 750, loss is 0.23547834\n",
      "epoch: 4 step: 875, loss is 0.23788\n",
      "epoch: 4 step: 1000, loss is 0.095217384\n",
      "epoch: 4 step: 1125, loss is 0.08334237\n",
      "epoch: 4 step: 1250, loss is 0.04447901\n",
      "epoch: 4 step: 1375, loss is 0.14708273\n",
      "epoch: 4 step: 1500, loss is 0.014412211\n",
      "epoch: 4 step: 1625, loss is 0.24204044\n",
      "epoch: 4 step: 1750, loss is 0.113967605\n",
      "epoch: 4 step: 1875, loss is 0.14672887\n",
      "epoch: 5 step: 125, loss is 0.48921666\n",
      "epoch: 5 step: 250, loss is 0.13006064\n",
      "epoch: 5 step: 375, loss is 0.1684221\n",
      "epoch: 5 step: 500, loss is 0.0022486271\n",
      "epoch: 5 step: 625, loss is 0.004598553\n",
      "epoch: 5 step: 750, loss is 0.14268123\n",
      "epoch: 5 step: 875, loss is 0.0027137673\n",
      "epoch: 5 step: 1000, loss is 0.13684595\n",
      "epoch: 5 step: 1125, loss is 0.3681666\n",
      "epoch: 5 step: 1250, loss is 0.061110772\n",
      "epoch: 5 step: 1375, loss is 0.31295025\n",
      "epoch: 5 step: 1500, loss is 0.2339364\n",
      "epoch: 5 step: 1625, loss is 0.14990948\n",
      "epoch: 5 step: 1750, loss is 0.101100326\n",
      "epoch: 5 step: 1875, loss is 0.0039413134\n"
     ]
    }
   ],
   "source": [
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor\n",
    "\n",
    "mnist_train_ds = create_dataset(mnist_train_dataset_path, 32)\n",
    "\n",
    "# 设置模型保存参数\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
    "# 应用模型保存参数\n",
    "ckpoint = ModelCheckpoint(prefix=\"checkpoint_lenet\", directory='./checkpoints', config=config_ck)\n",
    "\n",
    "model.train(epoch=5, train_dataset=mnist_train_ds, callbacks=[ckpoint, LossMonitor(125)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245aa43-5a67-46cd-84dd-8347b0fa41e7",
   "metadata": {},
   "source": [
    "Few things to note here:\n",
    "1. The `train_dataset`, expects the last column to be the labels. You can use `dataset.get_col_names()` to check which column is the lables.\n",
    "2. Each each instance of dataset can only be used once. Therefore we put the create_dataset in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c3a6c-6104-4730-899a-92642b9792cd",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403a9c2-523d-4c8f-af30-a3067177b9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
